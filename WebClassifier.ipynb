{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод первый - Явное нахождение название темы в словах\n",
    "###### когда никак не получается найти датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будем парсить страничку, оставляя только слова и глаголы и в них искать слова из данных тематик\n",
    "#### А выводить будем [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF) значения в процентах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T15:57:56.714109",
     "start_time": "2016-10-19T15:57:56.693151"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# данные тематики\n",
    "t = \"\"\"/Наука и образование\n",
    "    /Наука и образование/наука\n",
    "    /Наука и образование/наука/математика\n",
    "    /Наука и образование/наука/физика\n",
    "    /Наука и образование/наука/химия\n",
    "    /Наука и образование/наука/информатика\n",
    "    /Наука и образование/наука/информатика/биоинформатика\n",
    "    /Наука и образование/наука/информатика/анализ данных\n",
    "    /Наука и образование/наука/литература\n",
    "    /Наука и образование/образование\n",
    "    /Наука и образование/образование/школьное\n",
    "    /Наука и образование/образование/высшее\n",
    "    /Наука и образование/образование/дополнительное\n",
    "    /Наука и образование/образование/дополнительное/GoTo\n",
    "    /Политика\n",
    "    /Политика/Внутренняя\n",
    "    /Политика/Внешняя\n",
    "    /Экономика и бизнес\n",
    "    /Экономика и бизнес/Бизнес\n",
    "    /Экономика и бизнес/Бизнес/Стартапы\n",
    "    /Экономика и бизнес/Бизнес/Стартапы/E-Contenta\n",
    "    /Экономика и бизнес/Бизнес/Крупные компании\n",
    "    /Экономика и бизнес/Экономика\n",
    "    /Отдых и развлечения\n",
    "    /Отдых и развлечения/Кино\n",
    "    /Отдых и развлечения/Театр\n",
    "    /Отдых и развлечения/Компьютерные игры\n",
    "    /Здоровье и красота/Фитнес\n",
    "    /Здоровье и красота/Медицина\n",
    "    /Здоровье и красота/Косметология\"\"\".split(\"\\n\")\n",
    "\n",
    "# массив последних тем в иерархии (топики) и словарь, с пом. которого можно восстановить полную структуру\n",
    "topics = [x[1:].split(\"/\")[-1] for x in t]\n",
    "restore = {topics[i]: t[i] for i in range(len(topics))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T15:57:59.227505",
     "start_time": "2016-10-19T15:57:59.214504"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Наука и образование',\n",
       " 'наука',\n",
       " 'математика',\n",
       " 'физика',\n",
       " 'химия',\n",
       " 'информатика',\n",
       " 'биоинформатика',\n",
       " 'анализ данных',\n",
       " 'литература',\n",
       " 'образование',\n",
       " 'школьное',\n",
       " 'высшее',\n",
       " 'дополнительное',\n",
       " 'GoTo',\n",
       " 'Политика',\n",
       " 'Внутренняя',\n",
       " 'Внешняя',\n",
       " 'Экономика и бизнес',\n",
       " 'Бизнес',\n",
       " 'Стартапы',\n",
       " 'E-Contenta',\n",
       " 'Крупные компании',\n",
       " 'Экономика',\n",
       " 'Отдых и развлечения',\n",
       " 'Кино',\n",
       " 'Театр',\n",
       " 'Компьютерные игры',\n",
       " 'Фитнес',\n",
       " 'Медицина',\n",
       " 'Косметология']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T15:58:13.856988",
     "start_time": "2016-10-19T15:58:12.634652"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "from html2text import HTML2Text\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firstly, we need to download the page we are looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T16:04:32.049977",
     "start_time": "2016-10-19T16:04:31.574542"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "web_url = \"https://ru.wikipedia.org/wiki/%D0%9E%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C_%D0%92%D0%B0%D0%BD%D0%B4%D0%B5%D1%80%D0%BC%D0%BE%D0%BD%D0%B4%D0%B0\"\n",
    "page = requests.get(web_url)\n",
    "soup = BeautifulSoup(page.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step we clear text from punctuation, html tags and apply regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T16:04:37.821670",
     "start_time": "2016-10-19T16:04:37.818737"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = HTML2Text()\n",
    "h.ignore_links = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T16:04:39.123416",
     "start_time": "2016-10-19T16:04:39.107524"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_text = h.handle(soup.body.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T16:04:40.324615",
     "start_time": "2016-10-19T16:04:40.320308"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_russian_text = re.sub(\"[^а-яА-Я]\", \" \", raw_text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T16:08:34.857158",
     "start_time": "2016-10-19T16:08:34.843496"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nouns_list_from_url(web_url):\n",
    "    # получим текст\n",
    "    page = requests.get(web_url)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    h = HTML2Text()\n",
    "    h.ignore_links = True\n",
    "    raw_text = h.handle(soup.body.text)\n",
    "    only_russian_text = re.sub(\"[^а-яА-Я]\", \" \", raw_text).strip()\n",
    "    \n",
    "    # оставим только слова без стоп-слов длины [3, 15] и только существительные и глаголы в нормальной форме\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    words = only_russian_text.split()\n",
    "    stoplist = stopwords.words('russian')\n",
    "    data = []\n",
    "    for word in words:\n",
    "        if 2 < len(word) < 16 and word not in stoplist:\n",
    "            p = morph.parse(word)[0]\n",
    "            if p.tag.POS == 'NOUN' or \"VERB\":\n",
    "                data.append(str(p.normal_form))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T16:09:18.317616",
     "start_time": "2016-10-19T16:09:17.733903"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['определитель',\n",
       " 'вандермонд',\n",
       " 'материал',\n",
       " 'википедия',\n",
       " 'свободный',\n",
       " 'энциклопедия',\n",
       " 'перейти',\n",
       " 'навигация',\n",
       " 'поиск',\n",
       " 'определитель',\n",
       " 'вандермонд',\n",
       " 'называться',\n",
       " 'определитель',\n",
       " 'существовать',\n",
       " 'равно',\n",
       " 'степень',\n",
       " 'иной',\n",
       " 'слово',\n",
       " 'делиться',\n",
       " 'различный',\n",
       " 'многочлен',\n",
       " 'степень',\n",
       " 'значит',\n",
       " 'равный',\n",
       " 'произведение',\n",
       " 'точность',\n",
       " 'константа',\n",
       " 'убедиться',\n",
       " 'раскрыть',\n",
       " 'скобка',\n",
       " 'константа',\n",
       " 'равный',\n",
       " 'единица',\n",
       " 'дать',\n",
       " 'формула',\n",
       " 'показывать',\n",
       " 'определитель',\n",
       " 'вандермонд',\n",
       " 'равный',\n",
       " 'нуль',\n",
       " 'существовать',\n",
       " 'хотя',\n",
       " 'один',\n",
       " 'пара',\n",
       " 'такой',\n",
       " 'определитель',\n",
       " 'вандермонд',\n",
       " 'иметь',\n",
       " 'многочисленный',\n",
       " 'применение',\n",
       " 'разный',\n",
       " 'область',\n",
       " 'математика',\n",
       " 'например',\n",
       " 'решение',\n",
       " 'задача',\n",
       " 'интерполяция',\n",
       " 'многочлен',\n",
       " 'задача',\n",
       " 'нахождение',\n",
       " 'многочлен',\n",
       " 'степень',\n",
       " 'график',\n",
       " 'который',\n",
       " 'проходить',\n",
       " 'задать',\n",
       " 'точка',\n",
       " 'плоскость',\n",
       " 'абсцисса',\n",
       " 'определитель',\n",
       " 'вандермонд',\n",
       " 'возникать',\n",
       " 'определитель',\n",
       " 'система',\n",
       " 'линейный',\n",
       " 'уравнение',\n",
       " 'который',\n",
       " 'находиться',\n",
       " 'неизвестный',\n",
       " 'коэффициент',\n",
       " 'искомое',\n",
       " 'многочлен',\n",
       " 'матрица',\n",
       " 'вандермонд',\n",
       " 'представлять',\n",
       " 'себя',\n",
       " 'частный',\n",
       " 'случай',\n",
       " 'альтернативный',\n",
       " 'матрица',\n",
       " 'который',\n",
       " 'если',\n",
       " 'первообразный',\n",
       " 'корень',\n",
       " 'степень',\n",
       " 'единица',\n",
       " 'матрица',\n",
       " 'вандермонд',\n",
       " 'элемент',\n",
       " 'обратный',\n",
       " 'матрица',\n",
       " 'точность',\n",
       " 'диагональный',\n",
       " 'матрица',\n",
       " 'иметь',\n",
       " 'вид',\n",
       " 'литература',\n",
       " 'править',\n",
       " 'править',\n",
       " 'вика',\n",
       " 'текст',\n",
       " 'курош',\n",
       " 'курс',\n",
       " 'высокий',\n",
       " 'алгебра',\n",
       " 'наука',\n",
       " 'ильин',\n",
       " 'позняк',\n",
       " 'линейный',\n",
       " 'алгебра',\n",
       " 'наука',\n",
       " 'физматлить',\n",
       " 'шафаре',\n",
       " 'ремиз',\n",
       " 'линейный',\n",
       " 'алгебра',\n",
       " 'геометрия',\n",
       " 'физматлить',\n",
       " 'москва',\n",
       " 'примечание',\n",
       " 'править',\n",
       " 'править',\n",
       " 'вика',\n",
       " 'текст',\n",
       " 'русый',\n",
       " 'страница',\n",
       " 'шафаре',\n",
       " 'ремиз',\n",
       " 'линейный',\n",
       " 'алгебра',\n",
       " 'геометрия',\n",
       " 'пара',\n",
       " 'физматлить',\n",
       " 'москва',\n",
       " 'источник',\n",
       " 'определитель',\n",
       " 'вандермонд',\n",
       " 'категория',\n",
       " 'метод',\n",
       " 'линейный',\n",
       " 'алгебра',\n",
       " 'навигация',\n",
       " 'персональный',\n",
       " 'инструмент',\n",
       " 'представиться',\n",
       " 'тный',\n",
       " 'записьвойти',\n",
       " 'пространство',\n",
       " 'статья',\n",
       " 'обсуждение',\n",
       " 'вариант',\n",
       " 'просмотр',\n",
       " 'читать',\n",
       " 'править',\n",
       " 'править',\n",
       " 'вика',\n",
       " 'текст',\n",
       " 'история',\n",
       " 'поиск',\n",
       " 'навигация',\n",
       " 'заглавный',\n",
       " 'яизбранный',\n",
       " 'статьислучайный',\n",
       " 'статьятекущий',\n",
       " 'событие',\n",
       " 'участие',\n",
       " 'сообщить',\n",
       " 'ошибкепортал',\n",
       " 'правкиновое',\n",
       " 'инструмент',\n",
       " 'ссылка',\n",
       " 'сюдасвязанный',\n",
       " 'ссылкасведение',\n",
       " 'страницеэлемент',\n",
       " 'страница',\n",
       " 'печать',\n",
       " 'экспорт',\n",
       " 'создать',\n",
       " 'книгускачать',\n",
       " 'версия',\n",
       " 'печать',\n",
       " 'другой',\n",
       " 'язык',\n",
       " 'укр',\n",
       " 'нська',\n",
       " 'править',\n",
       " 'ссылка',\n",
       " 'последний',\n",
       " 'изменение',\n",
       " 'страница',\n",
       " 'январь',\n",
       " 'текст',\n",
       " 'доступный',\n",
       " 'лицензия',\n",
       " 'отдельный',\n",
       " 'случай',\n",
       " 'мочь',\n",
       " 'действовать',\n",
       " 'дополнительный',\n",
       " 'условие',\n",
       " 'дробный',\n",
       " 'условие',\n",
       " 'использование',\n",
       " 'товарный',\n",
       " 'знак',\n",
       " 'некоммерческий',\n",
       " 'организация',\n",
       " 'связаться',\n",
       " 'мы',\n",
       " 'политика',\n",
       " 'описание',\n",
       " 'википедия',\n",
       " 'отказ',\n",
       " 'ответственность',\n",
       " 'разработчик',\n",
       " 'соглашение',\n",
       " 'мобильный',\n",
       " 'версия',\n",
       " 'шаблон',\n",
       " 'шаблон',\n",
       " 'шаблон',\n",
       " 'шаблон',\n",
       " 'шаблон',\n",
       " 'шаблон',\n",
       " 'чтд',\n",
       " 'шаблон']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nouns_list_from_url(web_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time to select only nouns and turn them into normal form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:35:01.679646",
     "start_time": "2016-10-19T17:35:01.581416"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to get top tf-idf values and show corresponding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:35:02.519489",
     "start_time": "2016-10-19T17:35:02.515199"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# инициализируем модельки\n",
    "count_vect = CountVectorizer()\n",
    "tf_idf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:35:03.997022",
     "start_time": "2016-10-19T17:35:03.438898"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# применяем CountVectorizer к строке из слов, вытянутых с веб-страницы\n",
    "X = count_vect.fit_transform([\" \".join(get_nouns_list_from_url(web_url))])\n",
    "X = tf_idf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:35:04.669345",
     "start_time": "2016-10-19T17:35:04.663146"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# отсортированный по невозрастанию массив пар индекс-tfidf значение\n",
    "tfidf_repr = sorted([(a[0][1], a[1]) for a in X.todok().items()], key=lambda a: a[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:35:05.251094",
     "start_time": "2016-10-19T17:35:05.245448"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(75, 0.32551538350846376),\n",
       " (3, 0.32551538350846376),\n",
       " (149, 0.28482596056990578),\n",
       " (92, 0.28482596056990578),\n",
       " (1, 0.20344711469278987),\n",
       " (52, 0.20344711469278987),\n",
       " (47, 0.20344711469278987),\n",
       " (127, 0.16275769175423188),\n",
       " (55, 0.16275769175423188),\n",
       " (133, 0.16275769175423188)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_repr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:35:10.689298",
     "start_time": "2016-10-19T17:35:10.686013"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# теперь сделаем из этого словарь\n",
    "tfidf_dict = {id_ : tfidf for id_, tfidf in tfidf_repr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:35:14.746031",
     "start_time": "2016-10-19T17:35:14.742025"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# словарь для доставания слов из CountVectorizer словаря - то, что имеем\n",
    "good_vocab = {}\n",
    "for word, _id in count_vect.vocabulary_.items():\n",
    "    good_vocab[str(_id)] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:50:55.157997",
     "start_time": "2016-10-19T17:50:55.153721"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# хочется, чтобы сумма tf-idf значений в процентах была 100 :)\n",
    "def softmax(X):\n",
    "    a = np.array(X)\n",
    "    multiplier = 100 / np.sum(a)\n",
    "    return list(a * multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:55:42.454551",
     "start_time": "2016-10-19T17:55:42.430978"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "a = []\n",
    "b = []\n",
    "for k in topics:\n",
    "    # сначала название топика приведем к нормальной форме, чтобы лучше найти его в count_vect\n",
    "    word = morph.parse(k)[0].normal_form\n",
    "    _id = count_vect.vocabulary_.get(word)\n",
    "    if _id:\n",
    "        # по слову - айдишник, по айдишнику - tfidf значение. Если оно существует, то добавляем топик и tfidf в соотв. массивы\n",
    "        tfidf = tfidf_dict.get(_id)\n",
    "        if tfidf:\n",
    "            a.append(restore[k])\n",
    "            b.append(tfidf)\n",
    "b_soft = softmax(b)\n",
    "# только сейчас добавляем в результирующий массив изначальное название топика и его процентное содержание\n",
    "for i in range(len(a)):\n",
    "    res.append(\"{} — {:.2f}%\".format(a[i], b_soft[i]))\n",
    "    \n",
    "res = sorted(res, key=lambda x: float(x.split(\" — \")[1][:-1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T17:55:45.014276",
     "start_time": "2016-10-19T17:55:45.010523"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    /Наука и образование/наука — 28.57%',\n",
       " '    /Наука и образование/наука/математика — 14.29%',\n",
       " '    /Наука и образование/наука/литература — 14.29%',\n",
       " '    /Наука и образование/образование/высшее — 14.29%',\n",
       " '    /Наука и образование/образование/дополнительное — 14.29%',\n",
       " '    /Политика — 14.29%']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create all-in-one function (besause we can :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:56:21.123139",
     "start_time": "2016-10-19T20:56:20.970312"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "from html2text import HTML2Text\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "\n",
    "def method1(web_url, topn=10):\n",
    "    t = \"\"\"/Наука и образование\n",
    "    /Наука и образование/наука\n",
    "    /Наука и образование/наука/математика\n",
    "    /Наука и образование/наука/физика\n",
    "    /Наука и образование/наука/химия\n",
    "    /Наука и образование/наука/информатика\n",
    "    /Наука и образование/наука/информатика/биоинформатика\n",
    "    /Наука и образование/наука/информатика/анализ данных\n",
    "    /Наука и образование/наука/литература\n",
    "    /Наука и образование/образование\n",
    "    /Наука и образование/образование/школьное\n",
    "    /Наука и образование/образование/высшее\n",
    "    /Наука и образование/образование/дополнительное\n",
    "    /Наука и образование/образование/дополнительное/GoTo\n",
    "    /Политика\n",
    "    /Политика/Внутренняя\n",
    "    /Политика/Внешняя\n",
    "    /Экономика и бизнес\n",
    "    /Экономика и бизнес/Бизнес\n",
    "    /Экономика и бизнес/Бизнес/Стартапы\n",
    "    /Экономика и бизнес/Бизнес/Стартапы/E-Contenta\n",
    "    /Экономика и бизнес/Бизнес/Крупные компании\n",
    "    /Экономика и бизнес/Экономика\n",
    "    /Отдых и развлечения\n",
    "    /Отдых и развлечения/Кино\n",
    "    /Отдых и развлечения/Театр\n",
    "    /Отдых и развлечения/Компьютерные игры\n",
    "    /Здоровье и красота/Фитнес\n",
    "    /Здоровье и красота/Медицина\n",
    "    /Здоровье и красота/Косметология\"\"\".split(\"\\n\")\n",
    "    topics = [x[1:].split(\"/\")[-1] for x in t]\n",
    "    restore = {topics[i]: t[i] for i in range(len(topics))}\n",
    "    \n",
    "    \n",
    "    # получим текст\n",
    "    page = requests.get(web_url)\n",
    "    soup = BeautifulSoup(page.content, \"lxml\")\n",
    "    h = HTML2Text()\n",
    "    h.ignore_links = True\n",
    "    raw_text = h.handle(soup.body.text)\n",
    "    only_russian_text = re.sub(\"[^а-яА-Я]\", \" \", raw_text).strip()\n",
    "    \n",
    "    # оставим только слова без стоп-слов длины [3, 15] и только существительные и глаголы в нормальной форме\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    words = only_russian_text.split()\n",
    "    stoplist = stopwords.words('russian')\n",
    "    data = []\n",
    "    for word in words:\n",
    "        if 2 < len(word) < 16 and word not in stoplist:\n",
    "            p = morph.parse(word)[0]\n",
    "            if p.tag.POS == 'NOUN' or \"VERB\":\n",
    "                data.append(str(p.normal_form))\n",
    "                \n",
    "    count_vect = CountVectorizer()\n",
    "    tf_idf = TfidfTransformer()\n",
    "    X = count_vect.fit_transform([\" \".join(data)])\n",
    "    X = tf_idf.fit_transform(X)\n",
    "    \n",
    "    tfidf_repr = sorted([(a[0][1], a[1]) for a in X.todok().items()], key=lambda a: a[1], reverse=True)\n",
    "    tfidf_dict = {id_ : tfidf for id_, tfidf in tfidf_repr}\n",
    "    \n",
    "    good_vocab = {str(_id) : word for word, _id in count_vect.vocabulary_.items()}\n",
    "    \n",
    "    res = []\n",
    "    a = []\n",
    "    b = []\n",
    "    for k in topics:\n",
    "        # сначала название топика приведем к нормальной форме, чтобы лучше найти его в count_vect\n",
    "        word = morph.parse(k)[0].normal_form\n",
    "        _id = count_vect.vocabulary_.get(word)\n",
    "        if _id:\n",
    "            # по слову - айдишник, по айдишнику - tfidf значение. Если оно существует, то добавляем топик и tfidf в соотв. массивы\n",
    "            tfidf = tfidf_dict.get(_id)\n",
    "            if tfidf:\n",
    "                a.append(restore[k])\n",
    "                b.append(tfidf)\n",
    "                \n",
    "    v = np.array(b)\n",
    "    if np.sum(v) == 0:\n",
    "        print(\"Вероятно, вы показываете мне английский текст, я так не умею\")\n",
    "        print(\"Ну или вы нашли сайт, который не подходит ни под одну категорию\")\n",
    "        return []\n",
    "    multiplier = 100 / np.sum(v)\n",
    "    b_soft = list(v * multiplier)\n",
    "\n",
    "    # только сейчас добавляем в результирующий массив изначальное название топика и его процентное содержание\n",
    "    for i in range(len(a)):\n",
    "        res.append(\"{} — {:.2f}%\".format(a[i], b_soft[i]))\n",
    "        \n",
    "    return sorted(res, key=lambda x: float(x.split(\" — \")[1][:-1]), reverse=True)[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:56:22.859091",
     "start_time": "2016-10-19T20:56:22.275576"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    /Наука и образование/образование — 31.58%',\n",
       " '    /Наука и образование/наука — 15.79%',\n",
       " '    /Здоровье и красота/Медицина — 15.79%',\n",
       " '    /Экономика и бизнес/Бизнес — 10.53%',\n",
       " '    /Наука и образование/образование/школьное — 5.26%',\n",
       " '    /Экономика и бизнес/Экономика — 5.26%',\n",
       " '    /Отдых и развлечения/Кино — 5.26%',\n",
       " '    /Отдых и развлечения/Театр — 5.26%',\n",
       " '    /Здоровье и красота/Фитнес — 5.26%']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method1(\"http://goto.msk.ru/hackathon/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:56:25.938115",
     "start_time": "2016-10-19T20:56:22.978977"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    /Отдых и развлечения/Кино — 75.00%',\n",
       " '    /Наука и образование/образование/высшее — 12.50%',\n",
       " '    /Наука и образование/образование — 6.25%',\n",
       " '    /Политика/Внешняя — 6.25%']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method1(\"https://www.kinopoisk.ru/film/648440/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:56:27.923364",
     "start_time": "2016-10-19T20:56:27.248623"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    /Экономика и бизнес/Бизнес — 75.00%',\n",
       " '    /Наука и образование/образование/высшее — 25.00%']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method1(\"https://e-contenta.com/ru/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:56:32.455261",
     "start_time": "2016-10-19T20:56:28.609289"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    /Наука и образование/образование/высшее — 56.41%',\n",
       " '    /Наука и образование/образование/дополнительное — 10.26%',\n",
       " '    /Наука и образование/наука/математика — 7.69%',\n",
       " '    /Наука и образование/наука/литература — 7.69%',\n",
       " '    /Политика/Внешняя — 7.69%',\n",
       " '    /Наука и образование/наука/информатика — 5.13%',\n",
       " '    /Политика — 2.56%',\n",
       " '    /Политика/Внутренняя — 2.56%']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method1(\"https://ru.wikipedia.org/wiki/C%2B%2B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод 2 - Использовать Word2Vec модель\n",
    "#### Будем складывать векторы всех слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using [this](http://ling.go.mail.ru/misc/dialogue_2015.html) corpora\n",
    "### (Russian Wikipedia together with Russian National Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T18:07:11.018836",
     "start_time": "2016-10-19T18:07:08.435094"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:47:07.474134",
     "start_time": "2016-10-19T20:47:00.493266"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load_word2vec_format('news.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:47:07.502634",
     "start_time": "2016-10-19T20:47:07.479137"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "b = []\n",
    "for k in topics:\n",
    "    # сначала название топика приведем к нормальной форме, чтобы лучше найти его в count_vect\n",
    "    word = morph.parse(k)[0].normal_form\n",
    "    _id = count_vect.vocabulary_.get(word)\n",
    "    if _id:\n",
    "        # по слову - айдишник, по айдишнику - tfidf значение. Если оно существует, то добавляем топик и tfidf в соотв. массивы\n",
    "        tfidf = tfidf_dict.get(_id)\n",
    "        if tfidf:\n",
    "            a.append(k)\n",
    "            b.append(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:47:10.278574",
     "start_time": "2016-10-19T20:47:07.504132"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 0\n",
    "# нулевой вектор, он же и результирующий - сложение по правилам numpy\n",
    "vector_summ = np.zeros(model.vector_size)\n",
    "for i in range(len(a)):\n",
    "    try:\n",
    "        vector_summ += (model[a[i]] * b[i])\n",
    "    except KeyError:\n",
    "        k += 1\n",
    "\n",
    "hundred_similar_words = model.most_similar(positive=[vector_summ], negative=[], topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:47:12.382578",
     "start_time": "2016-10-19T20:47:12.373207"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('наука', 0.8318927884101868),\n",
       " ('математика', 0.6977936029434204),\n",
       " ('литература', 0.6834399700164795),\n",
       " ('физика', 0.6183444857597351),\n",
       " ('биология', 0.6048916578292847),\n",
       " ('естествознание', 0.5761309862136841),\n",
       " ('физико-математический', 0.5751566290855408),\n",
       " ('информатика', 0.5618166923522949),\n",
       " ('лингвистика', 0.5615713596343994),\n",
       " ('языкознание', 0.5526806116104126),\n",
       " ('культурология', 0.5335828065872192),\n",
       " ('филологический', 0.5316666960716248),\n",
       " ('обществознание', 0.531338632106781),\n",
       " ('химия', 0.5215914249420166),\n",
       " ('кибернетика', 0.5192195177078247),\n",
       " ('естественнонаучный', 0.517939031124115),\n",
       " ('обществоведение', 0.5163986682891846),\n",
       " ('литературоведение', 0.5140202641487122),\n",
       " ('член-корреспондент', 0.5103638172149658),\n",
       " ('астрономия', 0.5091902017593384),\n",
       " ('математик', 0.5085898041725159),\n",
       " ('алгебра', 0.501548707485199),\n",
       " ('педагогика', 0.49928444623947144),\n",
       " ('естественно-научный', 0.49542444944381714),\n",
       " ('доцент', 0.4946599304676056),\n",
       " ('география', 0.4937931299209595),\n",
       " ('профессор', 0.49146321415901184),\n",
       " ('филология', 0.49140119552612305),\n",
       " ('преподавание', 0.4779626429080963),\n",
       " ('искусствоведение', 0.4751476049423218),\n",
       " ('религиоведение', 0.4680687189102173),\n",
       " ('научный', 0.4677838683128357),\n",
       " ('доктор', 0.4668896794319153),\n",
       " ('гуманитарий', 0.4645380675792694),\n",
       " ('рггу', 0.4618639349937439),\n",
       " ('биофизика', 0.45989352464675903),\n",
       " ('кафедра', 0.4597598612308502),\n",
       " ('икт', 0.4474126696586609),\n",
       " ('егэ', 0.44584789872169495),\n",
       " ('материаловедение', 0.44132643938064575),\n",
       " ('философский', 0.44089844822883606),\n",
       " ('постдипломный', 0.4373588562011719),\n",
       " ('этнология', 0.43615925312042236),\n",
       " ('зоология', 0.43352681398391724),\n",
       " ('ран', 0.43237361311912537),\n",
       " ('фольклористика', 0.43184441328048706),\n",
       " ('словесность', 0.43153420090675354),\n",
       " ('выпускник', 0.43024584650993347),\n",
       " ('регионоведение', 0.4299991726875305),\n",
       " ('факультет', 0.4281841516494751),\n",
       " ('завкафедрой', 0.42376160621643066),\n",
       " ('экзамен', 0.42358309030532837),\n",
       " ('информатик', 0.4235752820968628),\n",
       " ('богословие', 0.42037975788116455),\n",
       " ('междисциплинарный', 0.4161747694015503),\n",
       " ('математический', 0.4150702655315399),\n",
       " ('декан', 0.41352152824401855),\n",
       " ('агрономия', 0.4131290912628174),\n",
       " ('кристаллография', 0.41259342432022095),\n",
       " ('почвоведение', 0.4124391973018646),\n",
       " ('биоинформатика', 0.41017675399780273),\n",
       " ('антропология', 0.4081488847732544),\n",
       " ('последипломный', 0.4080609977245331),\n",
       " ('педагогический', 0.4073807895183563),\n",
       " ('политология', 0.4067690372467041),\n",
       " ('монография', 0.4064355790615082),\n",
       " ('образование', 0.4056474566459656),\n",
       " ('физфак', 0.4029153883457184),\n",
       " ('теология', 0.4027196764945984),\n",
       " ('преподавать', 0.40110549330711365),\n",
       " ('механико-математический', 0.40066710114479065),\n",
       " ('стобалльник', 0.40048903226852417),\n",
       " ('философия', 0.39983850717544556),\n",
       " ('биохимия', 0.3990231156349182),\n",
       " ('преподаватель', 0.3987430930137634),\n",
       " ('ргпу', 0.3985227644443512),\n",
       " ('спбга', 0.3980093002319336),\n",
       " ('минералогический', 0.39568665623664856),\n",
       " ('преподаваться', 0.39553332328796387),\n",
       " ('краеведение', 0.39509719610214233),\n",
       " ('одиннадцатиклассник', 0.3929375112056732),\n",
       " ('физмат', 0.3927803635597229),\n",
       " ('академия', 0.3923981189727783),\n",
       " ('вузовский', 0.3916284441947937),\n",
       " ('источниковедение', 0.3908005356788635),\n",
       " ('страноведение', 0.3902626633644104),\n",
       " ('мехмат', 0.38946956396102905),\n",
       " ('докторант', 0.38823267817497253),\n",
       " ('институт', 0.38750046491622925),\n",
       " ('психология', 0.3869078755378723),\n",
       " ('минобрнаука', 0.38573724031448364),\n",
       " ('документоведение', 0.38497424125671387),\n",
       " ('инноватик', 0.38430291414260864),\n",
       " ('школьник', 0.38413113355636597),\n",
       " ('членкор', 0.3827853500843048),\n",
       " ('филолог', 0.3826480507850647),\n",
       " ('академик', 0.38262739777565),\n",
       " ('популяризатор', 0.3823971748352051),\n",
       " ('востоковедение', 0.38145485520362854),\n",
       " ('минобразование', 0.38070210814476013)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hundred_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:47:13.565688",
     "start_time": "2016-10-19T20:47:13.561483"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('    /Наука и образование/наука', 0.8318927884101868)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restore[thousand_similar_words[0][0]], thousand_similar_words[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:47:15.919790",
     "start_time": "2016-10-19T20:47:15.914263"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "try:\n",
    "    for i in range(len(restore)):\n",
    "        similar_word = thousand_similar_words[i]\n",
    "        res.append(\"{} — {:.2f}%\".format(restore[similar_word[0]], similar_word[1] * 100))\n",
    "except KeyError:\n",
    "    pass\n",
    "    \n",
    "res = sorted(res, key=lambda x: float(x.split(\" — \")[1][:-1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:47:16.974760",
     "start_time": "2016-10-19T20:47:16.971098"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    /Наука и образование/наука — 83.19%',\n",
       " '    /Наука и образование/наука/математика — 69.78%',\n",
       " '    /Наука и образование/наука/литература — 68.34%',\n",
       " '    /Наука и образование/наука/физика — 61.83%']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Снова объединим все блоки и определим класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-20T18:50:36.821940",
     "start_time": "2016-10-20T18:50:36.613937"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "from html2text import HTML2Text\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "class Method2():\n",
    "    def __init__(self):\n",
    "        t = \"\"\"/Наука и образование\n",
    "        /Наука и образование/наука\n",
    "        /Наука и образование/наука/математика\n",
    "        /Наука и образование/наука/физика\n",
    "        /Наука и образование/наука/химия\n",
    "        /Наука и образование/наука/информатика\n",
    "        /Наука и образование/наука/информатика/биоинформатика\n",
    "        /Наука и образование/наука/информатика/анализ данных\n",
    "        /Наука и образование/наука/литература\n",
    "        /Наука и образование/образование\n",
    "        /Наука и образование/образование/школьное\n",
    "        /Наука и образование/образование/высшее\n",
    "        /Наука и образование/образование/дополнительное\n",
    "        /Наука и образование/образование/дополнительное/GoTo\n",
    "        /Политика\n",
    "        /Политика/Внутренняя\n",
    "        /Политика/Внешняя\n",
    "        /Экономика и бизнес\n",
    "        /Экономика и бизнес/Бизнес\n",
    "        /Экономика и бизнес/Бизнес/Стартапы\n",
    "        /Экономика и бизнес/Бизнес/Стартапы/E-Contenta\n",
    "        /Экономика и бизнес/Бизнес/Крупные компании\n",
    "        /Экономика и бизнес/Экономика\n",
    "        /Отдых и развлечения\n",
    "        /Отдых и развлечения/Кино\n",
    "        /Отдых и развлечения/Театр\n",
    "        /Отдых и развлечения/Компьютерные игры\n",
    "        /Здоровье и красота/Фитнес\n",
    "        /Здоровье и красота/Медицина\n",
    "        /Здоровье и красота/Косметология\"\"\".split(\"\\n\")\n",
    "        self.topics = [x[1:].split(\"/\")[-1] for x in t]\n",
    "        self.restore = {self.topics[i]: t[i] for i in range(len(self.topics))}\n",
    "\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "        self.model = Word2Vec.load_word2vec_format('ruscorpora.model.bin', binary=True)\n",
    "\n",
    "    def classify(self, web_url, topn=10):\n",
    "\n",
    "        # получим текст\n",
    "        page = requests.get(web_url)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        h = HTML2Text()\n",
    "        h.ignore_links = True\n",
    "        raw_text = h.handle(soup.body.text)\n",
    "        only_russian_text = re.sub(\"[^а-яА-Я]\", \" \", raw_text).strip()\n",
    "    \n",
    "        # оставим только слова без стоп-слов длины [3, 15] и только существительные и глаголы в нормальной форме\n",
    "    \n",
    "        words = only_russian_text.split()\n",
    "        stoplist = stopwords.words('russian')\n",
    "        data = []\n",
    "        for word in words:\n",
    "            if 2 < len(word) < 16 and word not in stoplist:\n",
    "                p = self.morph.parse(word)[0]\n",
    "                if p.tag.POS == 'NOUN' or \"VERB\":\n",
    "                    data.append(str(p.normal_form))\n",
    "    \n",
    "        count_vect = CountVectorizer()\n",
    "        tf_idf = TfidfTransformer()\n",
    "        X = count_vect.fit_transform([\" \".join(data)])\n",
    "        X = tf_idf.fit_transform(X)\n",
    "    \n",
    "        tfidf_repr = sorted([(a[0][1], a[1]) for a in X.todok().items()], key=lambda a: a[1], reverse=True)\n",
    "        tfidf_dict = {id_: tfidf for id_, tfidf in tfidf_repr}\n",
    "    \n",
    "        a = []\n",
    "        b = []\n",
    "        for k in self.topics:\n",
    "            # сначала название топика приведем к нормальной форме, чтобы лучше найти его в count_vect\n",
    "            word = self.morph.parse(k)[0].normal_form\n",
    "            _id = count_vect.vocabulary_.get(word)\n",
    "            if _id:\n",
    "                # по слову - айдишник, по айдишнику - tfidf значение. Если оно существует, то добавляем топик и tfidf в соотв. массивы\n",
    "                tfidf = tfidf_dict.get(_id)\n",
    "                if tfidf:\n",
    "                    a.append(k)\n",
    "                    b.append(tfidf)\n",
    "    \n",
    "        # нулевой вектор, он же и результирующий - сложение по правилам numpy\n",
    "        vector_summ = np.zeros(self.model.vector_size)\n",
    "        for i in range(len(a)):\n",
    "            try:\n",
    "                vector_summ += (self.model[a[i]] * b[i])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    \n",
    "        hundred_similar_words = self.model.most_similar(positive=[vector_summ], negative=[], topn=1000)\n",
    "    \n",
    "        res = []\n",
    "        try:\n",
    "            for i in range(len(self.restore)):\n",
    "                similar_word = hundred_similar_words[i]\n",
    "                res.append(\"{} — {:.2f}%\".format(self.restore[similar_word[0]], similar_word[1] * 100))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "        return sorted(res, key=lambda x: float(x.split(\" — \")[1][:-1]), reverse=True)[:topn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-20T18:50:47.381889",
     "start_time": "2016-10-20T18:50:37.489511"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Method2()\n",
    "method2 = lambda web_url: m.classify(web_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:55:37.593670",
     "start_time": "2016-10-19T20:55:35.564996"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['        /Наука и образование/образование — 91.41%',\n",
       " '        /Наука и образование/наука — 58.53%']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method2(\"http://goto.msk.ru/hackathon/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:55:42.517478",
     "start_time": "2016-10-19T20:55:39.297596"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method2(\"https://www.kinopoisk.ru/film/648440/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-19T20:55:51.899752",
     "start_time": "2016-10-19T20:55:51.227874"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method2(\"https://e-contenta.com/ru/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-20T18:50:54.796285",
     "start_time": "2016-10-20T18:50:54.272084"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['        /Наука и образование/наука — 90.71%',\n",
       " '        /Наука и образование/наука/математика — 68.50%',\n",
       " '        /Наука и образование/наука/литература — 67.38%']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method2(\"https://ru.wikipedia.org/wiki/%D0%9E%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C_%D0%92%D0%B0%D0%BD%D0%B4%D0%B5%D1%80%D0%BC%D0%BE%D0%BD%D0%B4%D0%B0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод 3 - Использование линейной модели\n",
    "###  Датасет с http://www.dmoz.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans(num_clusters, dataSet):\n",
    "    # n_clusters: number of centroids\n",
    "    # n_jobs: number of jobs running in parallel\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "   \n",
    "    kmeansClustering = KMeans(n_clusters=num_clusters)\n",
    "    # Compute cluster centers and predict cluster index for each sample\n",
    "    centroidIndx = kmeansClustering.fit_predict(dataSet)\n",
    "\n",
    "    return centroidIndx\n",
    "\n",
    "def create_bag_of_centroids(reviewData,num_clusters,index_word_map):\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    \n",
    "    \"\"\"\n",
    "        assign each word in the review to a centroid\n",
    "        this returns a numpy array with the dimension as num_clusters\n",
    "        each will be served as one feature for classification\n",
    "        :param reviewData:\n",
    "        :return:\n",
    "    \"\"\"\n",
    "    featureVector = np.zeros(num_clusters, dtype=np.float)\n",
    "    \n",
    "    for word in reviewData:\n",
    "        if word in index_word_map:\n",
    "            index = index_word_map[word]\n",
    "            featureVector[index] += 1\n",
    "    \n",
    "    return featureVector\n",
    "\n",
    "#and here is the function for the classifier\n",
    "def rfClassifer(n_estimators, trainingSet, label, testSet):\n",
    "    \n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    \n",
    "    forest = RandomForestClassifier(n_estimators)\n",
    "    forest = forest.fit(trainingSet, label)\n",
    "    result = forest.predict(testSet)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilities.preProc as preProc\n",
    "import utilities.classifierFuncs as cfun\n",
    "\n",
    "modelName = \"../../classifier/Word2VectforNLPTraining\"\n",
    "model = Word2Vec.load(open(modelName,\"rb\"))\n",
    "\n",
    "# model.init_sims(replace=True)\n",
    "\n",
    "wordVectors = model.syn0\n",
    "# print(wordVectors[0])\n",
    "num_clusters = int(wordVectors.shape[0] / 5)\n",
    "# print(\"number of clusters: {}\".format(num_clusters))\n",
    "# input(\"Press enter to continue:\")\n",
    "print(\"Clustering...\")\n",
    "startTime = time.time()\n",
    "clusterIndex = cfun.kmeans(num_clusters, wordVectors)\n",
    "endTime = time.time()\n",
    "\n",
    "print(\"Time taken for clustering: {} seconds\".format(endTime - startTime))\n",
    "\n",
    "clusterf = open(\"../../classifier/doc2vec/clusterIndex.pickle\",\"wb\") \n",
    "\n",
    "pickle.dump(clusterIndex,clusterf)\n",
    "\n",
    "# create a word/index dictionary, mapping each vocabulary word to a cluster number\n",
    "# zip(): make an iterator that aggregates elements from each of the iterables\n",
    "index_word_map = dict(zip(model.index2word, clusterIndex))\n",
    "\n",
    "train = pd.read_csv(\"../../data/labeledTrainData.tsv\",\n",
    "                header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"../../data/testData.tsv\",\n",
    "               header=0, delimiter=\"\\t\", quoting=3)\n",
    "\n",
    "trainingDataFV = np.zeros((train[\"review\"].size, num_clusters), dtype=np.float)\n",
    "testDataFV = np.zeros((test[\"review\"].size, num_clusters), dtype=np.float)\n",
    "\n",
    "# We dont really need to clean the data as the junk terms will be ignored anyway. This is due to the fact that we did not consider these while creating the model\n",
    "# and hence they will not feature in the model's vocabulary. Still this step will expedite the classification and feature vector creation.\n",
    "print(\"Processing training data...\")\n",
    "counter = 0\n",
    "cleanedTrainingData = preProc.clean_data(train)\n",
    "for review in cleanedTrainingData:\n",
    "    trainingDataFV[counter] = cfun.create_bag_of_centroids(review,num_clusters,index_word_map)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Processing test data...\")\n",
    "counter = 0\n",
    "cleaned_test_data = preProc.clean_data(test)\n",
    "for review in cleaned_test_data:\n",
    "    testDataFV[counter] = cfun.create_bag_of_centroids(review,num_clusters,index_word_map)\n",
    "    counter += 1\n",
    "\n",
    "n_estimators = 100\n",
    "result = cfun.rfClassifer(n_estimators, trainingDataFV, train[\"sentiment\"],testDataFV)\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": result})\n",
    "output.to_csv(\"Word2Vec_Clustering.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### Now we got data for vectorizer, let's create SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T12:27:25.509065",
     "start_time": "2016-10-17T12:27:25.498609"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stopwords.words('russian'))\n",
    "hash_vect = HashingVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "sgd_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', count_vect),\n",
    "                     ('tfidf', tfidf_transformer),\n",
    "                     ('clf', sgd_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for web_url, genre in ...:\n",
    "    X.append(get_text_from_url(web_url))\n",
    "    Y.append(genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf.fit(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
